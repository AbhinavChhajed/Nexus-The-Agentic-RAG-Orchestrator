{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import base64\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "import mimetypes\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import dotenv\n",
    "import os\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "import requests\n",
    "import json\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from typing import Literal\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",temperature = 0)\n",
    "\n",
    "class UniversalLoader:\n",
    "    def __init__(self,llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def process_file(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Traffic Controller: Routes files to the correct reader.\n",
    "        \"\"\"\n",
    "        # 1. Get extension and mime type\n",
    "        _, ext = os.path.splitext(file_path)\n",
    "        ext = ext.lower()\n",
    "        mime_type, _ = mimetypes.guess_type(file_path)\n",
    "        \n",
    "        # 2. DEFINE CODE EXTENSIONS (Treat these as text)\n",
    "        code_extensions = {'.py', '.js', '.ts', '.html', '.css', '.java', '.cpp', '.c', '.h', '.sql', '.md', '.json', '.xml', '.yaml', '.yml', '.txt'}\n",
    "\n",
    "        if ext in code_extensions:\n",
    "            return self._process_code(file_path, ext)\n",
    "        \n",
    "        elif mime_type and \"pdf\" in mime_type:\n",
    "            return self._process_pdf(file_path)\n",
    "        \n",
    "        elif mime_type and \"csv\" in mime_type:\n",
    "            return self._process_csv(file_path)\n",
    "        \n",
    "        elif mime_type and \"image\" in mime_type:\n",
    "            return self._process_image(file_path)\n",
    "        \n",
    "        else:\n",
    "            return f\"Unsupported file type: {mime_type or ext}\"\n",
    "        \n",
    "    def _process_code(self, file_path, ext):\n",
    "        \"\"\"Reads code files and wraps them in markdown.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "            # Wrap in markdown so LLM knows it's code\n",
    "            lang_map = {'.py': 'python', '.js': 'javascript', '.ts': 'typescript', '.html': 'html', '.sql': 'sql', '.css': 'css'}\n",
    "            language = lang_map.get(ext, '')\n",
    "            return f\"```{language}\\n{content}\\n```\"\n",
    "        except Exception as e:\n",
    "            return f\"Error reading code file: {e}\"\n",
    "        \n",
    "    def _process_txt(self,file_path):\n",
    "        with open(file_path,'r') as f:\n",
    "            return f.read()\n",
    "        \n",
    "    def _process_pdf(self,file_path):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        pages = loader.load_and_split()\n",
    "        result = \"\"\n",
    "        for page in pages:\n",
    "            result += page.page_content + \"\\n\"\n",
    "        return result\n",
    "    \n",
    "    def _process_csv(self,file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df.to_markdown(index=False)\n",
    "    \n",
    "    def _process_image(self,file_path):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                    \n",
    "                image_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "                \n",
    "                Prompt = HumanMessage(content = [\n",
    "                    {\"type\":\"text\",\"text\":\"Describe the following image in detail.\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}},\n",
    "                ])\n",
    "                response = self.llm.invoke([Prompt])\n",
    "                return response.content\n",
    "\n",
    "        except Exception as e:\n",
    "            return \"error processing image: \" + str(e)\n",
    "universalloader = UniversalLoader(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee647b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "vector_store = FAISS.from_texts([\"Nexus Initialized\"], embeddings)\n",
    "\n",
    "def index_files(file_paths):\n",
    "    \"\"\"\n",
    "    Reads files -> Chunks them -> Saves to Vector DB (Locally)\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    for path in file_paths:\n",
    "        print(f\"Loading: {path}...\")\n",
    "        \n",
    "        # Extract Text using your UniversalLoader\n",
    "        raw_content = universalloader.process_file(path)\n",
    "        \n",
    "        # Convert to Document\n",
    "        doc = Document(page_content=raw_content, metadata={\"source\": path})\n",
    "        all_documents.append(doc)\n",
    "        \n",
    "    # Split into chunks\n",
    "    splits = text_splitter.split_documents(all_documents)\n",
    "    \n",
    "    # Add to Vector Store\n",
    "    if splits:\n",
    "        vector_store.add_documents(splits)\n",
    "        print(f\"Successfully indexed {len(splits)} chunks locally!\")\n",
    "    else:\n",
    "        print(\"No content found to index.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec72d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "@tool\n",
    "def retrieve_documents(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search and retrieve information from internal documents, code, and policies.\n",
    "    Use this tool when the user asks about specific files or internal knowledge.\n",
    "    \"\"\"\n",
    "    pages = retriever.invoke(query)\n",
    "    result = \"\"\n",
    "    for page in pages:\n",
    "        result += page.page_content + \"\\n\\n\"\n",
    "    return result\n",
    "\n",
    "def should_continue(state: MessagesState) -> str:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM asks for a tool, go to \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, stop\n",
    "    return END\n",
    "\n",
    "# 1. The Approved Tools\n",
    "tools = [ retrieve_documents]\n",
    "\n",
    "# 2. Bind Tools to Model\n",
    "model = model.bind_tools(tools)\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# 3. Define the Agent Logic\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# 4. Build the Graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue)\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "print(\"Prototype Complete. Ready to migrate to Backend.\")\n",
    "\n",
    "def get_nexus_response(user_prompt: str, files: list = None):\n",
    "    \"\"\"\n",
    "    This function handles the AI logic.\n",
    "    Currently, it processes text. Later, we will add RAG (File) support here.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        if files and len(files)>0:\n",
    "            index_files(files)\n",
    "            file_names = \", \".join([os.path.basename(f) for f in files])\n",
    "            user_prompt = f\"System Note: The user just uploaded these files: {file_names}. \\n\\nUser Question: {user_prompt}\"\n",
    "        else:\n",
    "            user_prompt = f\"User Question: {user_prompt}\"\n",
    "\n",
    "        system_instruction = SystemMessage(content=\"\"\"\n",
    "        You are Nexus, an advanced AI with file-reading capabilities.\n",
    "        \n",
    "        CRITICAL RULES:\n",
    "        1. You have a tool named 'retrieve_documents'.\n",
    "        2. IF the user asks about \"the file\", \"uploaded documents\", or content you don't know:\n",
    "           YOU MUST USE 'retrieve_documents' to look it up.\n",
    "        3. DO NOT say \"I cannot access files\". You HAVE the tool. Use it.\n",
    "        4. If the tool returns text, assume it is the correct content of the file.\n",
    "        \"\"\")\n",
    "\n",
    "        config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "        inputs = {\"messages\": [system_instruction,HumanMessage(content=user_prompt)]}\n",
    "        result_state = app.invoke(inputs,config=config)\n",
    "        last_message = result_state['messages'][-1]\n",
    "        content = last_message.content\n",
    "       \n",
    "        if isinstance(content, list):\n",
    "            text_parts = []\n",
    "            for part in content:\n",
    "                # Extract 'text' if it exists, otherwise convert whole part to string\n",
    "                if isinstance(part, dict):\n",
    "                    text_parts.append(part.get('text', str(part)))\n",
    "                else:\n",
    "                    text_parts.append(str(part))\n",
    "            return \"\\n\".join(text_parts)\n",
    "        \n",
    "        # Case B: Content is already a string\n",
    "        if isinstance(content, str):\n",
    "            return content\n",
    "            \n",
    "        # Case C: Fallback\n",
    "        return str(content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"I encountered an error processing your request: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b50acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test file at: d:\\centeral folder\\Nexus-The-Agentic-RAG-Orchestrator\\testing\\secret_plans.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1. Create a dummy file for testing\n",
    "test_file_path = \"secret_plans.txt\"\n",
    "with open(test_file_path, \"w\") as f:\n",
    "    f.write(\"CONFIDENTIAL: The Project Nexus launch date is October 15th, 2025. The admin password is 'BlueSky99'.\")\n",
    "\n",
    "print(f\"Created test file at: {os.path.abspath(test_file_path)}\")\n",
    "\n",
    "# 2. Set API Key (Replace with your actual key if .env is not working)\n",
    "# Make sure .env is in the same folder or parent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c01e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model & Vector Store...\n",
      "ü§ñ Agent Ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "import mimetypes\n",
    "import pandas as pd\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "# --- 1. INITIALIZE ---\n",
    "print(\"Initializing Model & Vector Store...\")\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0) # Use 1.5-flash or 2.0-flash-exp\n",
    "embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "vector_store = FAISS.from_texts([\"Nexus Knowledge Base Initialized\"], embeddings)\n",
    "\n",
    "# --- 2. LOADER & INDEXER ---\n",
    "class UniversalLoader:\n",
    "    def process_file(self, file_path):\n",
    "        _, ext = os.path.splitext(file_path)\n",
    "        try:\n",
    "            if ext == \".txt\":\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f: return f.read()\n",
    "            elif ext == \".pdf\":\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                return \"\\n\".join([p.page_content for p in loader.load_and_split()])\n",
    "            else:\n",
    "                return \"Unsupported file type\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "loader = UniversalLoader()\n",
    "\n",
    "def index_files(file_paths):\n",
    "    docs = []\n",
    "    for path in file_paths:\n",
    "        content = loader.process_file(path)\n",
    "        docs.append(Document(page_content=content, metadata={\"source\": path}))\n",
    "    \n",
    "    if docs:\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "        vector_store.add_documents(splits)\n",
    "        print(f\"‚úÖ Successfully indexed {len(splits)} chunks from {file_paths}\")\n",
    "\n",
    "# --- 3. TOOLS ---\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "@tool\n",
    "def retrieve_documents(query: str) -> str:\n",
    "    \"\"\"Search uploaded files for information.\"\"\"\n",
    "    print(f\"üîç TOOL CALLED: retrieve_documents('{query}')\") \n",
    "    docs = retriever.invoke(query)\n",
    "    if not docs: return \"No info found.\"\n",
    "    return \"\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "tools = [search_tool, retrieve_documents]\n",
    "\n",
    "# --- 4. GRAPH ---\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    return {\"messages\": [model_with_tools.invoke(state['messages'])]}\n",
    "\n",
    "def should_continue(state: MessagesState):\n",
    "    if state['messages'][-1].tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue)\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "print(\"ü§ñ Agent Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7795afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully indexed 1 chunks from ['secret_plans.txt']\n",
      "\n",
      "üí¨ User: What is the admin password for Project Nexus?\n",
      "----------------------------------------\n",
      "ü§ñ Agent Thinking...\n",
      "üîç TOOL CALLED: retrieve_documents('admin password for Project Nexus')\n",
      "üõ†Ô∏è Tool Output Received\n",
      "ü§ñ Agent Thinking...\n",
      "----------------------------------------\n",
      "‚úÖ Final Answer:\n",
      "[{'type': 'text', 'text': \"The admin password for Project Nexus is 'BlueSky99'.\", 'extras': {'signature': 'CuABAXLI2nwLni1wyc8LrUlGHuVZmWEHJXxt60Iz1VagJ8G86+kaaz440G5VYEzdMN/5QPErM5uhlBR1IYsclQJbsSh682Ry8ICYkH9sWDIpuER4KWfclozWZ8IMjUfevwRhqA9hnmECPvTKy1Y5PzjIK3P9+Vs7mb4BaLLork6e+DqJgl0mh24xeeORHwyY4w/88mDxRoU15hy0W1ygAi1qr7PAVzKGHu+ON9aLKkqQmSlyrdkMxcLcvJlwzBE7c/WG6K0G7IvtOeabJES8C5UP0BnfDJWp8ordKz3kyU+8TIo='}}]\n"
     ]
    }
   ],
   "source": [
    "# --- RUN THE TEST ---\n",
    "\n",
    "# 1. Force Index the File\n",
    "index_files([\"secret_plans.txt\"])\n",
    "\n",
    "# 2. Define the Prompt\n",
    "user_query = \"What is the admin password for Project Nexus?\"\n",
    "\n",
    "# 3. Inject System Prompt to Force Tool Use\n",
    "system_msg = SystemMessage(content=\"\"\"\n",
    "You are Nexus. \n",
    "CRITICAL: You have access to a tool 'retrieve_documents'. \n",
    "If the user asks about 'Project Nexus', 'password', or 'launch date', YOU MUST use that tool.\n",
    "Do not say you cannot access files.\n",
    "\"\"\")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"test_notebook_1\"}}\n",
    "inputs = {\"messages\": [system_msg, HumanMessage(content=user_query)]}\n",
    "\n",
    "print(f\"\\nüí¨ User: {user_query}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 4. Run Agent\n",
    "for event in app.stream(inputs, config=config):\n",
    "    for key, value in event.items():\n",
    "        if key == \"agent\":\n",
    "            print(\"ü§ñ Agent Thinking...\")\n",
    "            # Uncomment to see raw thought: print(value['messages'][0].content)\n",
    "        elif key == \"tools\":\n",
    "            print(\"üõ†Ô∏è Tool Output Received\")\n",
    "\n",
    "# 5. Get Final Answer\n",
    "final_state = app.get_state(config)\n",
    "final_response = final_state.values['messages'][-1].content\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"‚úÖ Final Answer:\\n{final_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63bcf1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3dcf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing FastEmbed...\n",
      "‚úÖ Success! FastEmbed is working.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "print(\"Testing FastEmbed...\")\n",
    "# This line was crashing before. It should work now!\n",
    "embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "print(\"‚úÖ Success! FastEmbed is working.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
